
FROM spark:3.5.4-scala2.12-java11-ubuntu

USER root

RUN set -ex; \
    apt-get update; \
    apt-get install -y python3 python3-pip; \
    rm -rf /var/lib/apt/lists/*

ENV R_HOME /usr/lib/R

USER spark

# if wants to build from java base image
# Start from a base OpenJDK 11 image
# FROM openjdk:11-jdk
# # Arguments to parameterize Spark and Beam versions
# ARG SPARK_VERSION=3.5.4
# ARG BEAM_VERSION=2.48.0

# # Avoid interactive prompts during apt-get
# ENV DEBIAN_FRONTEND=noninteractive

# # 1. Install Python3, pip, curl
# RUN apt-get update && \
#     apt-get install -y python3 python3-pip curl && \
#     rm -rf /var/lib/apt/lists/*

# RUN rm -f /opt/spark
# # 2. Download & install Spark (pre-built without Hadoop)
# RUN curl -fSL "https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz" \
#     | tar -xz -C /opt && \
#     ln -s "/opt/spark-${SPARK_VERSION}-bin-hadoop3" /opt/spark

# Set Spark environment variables
#ENV SPARK_HOME=/opt/spark
#ENV PATH="$SPARK_HOME/bin:$PATH"

# 3. Install Apache Beam with Spark runner in Python
#RUN pip3 install --no-cache-dir "apache-beam[spark]==${BEAM_VERSION}"

# 4. Add SLF4J dependencies for Spark logging compatibility
# RUN rm -f /opt/spark/jars/slf4j-*.jar

# Default working directory
#WORKDIR $SPARK_HOME

# Default command: drop into a shell (override in Kubernetes YAML)
#CMD ["/bin/bash"]
